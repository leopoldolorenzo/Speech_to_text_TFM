#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
verificar_corpus_lm08_integridad.py

âœ… Verifica que el vocabulario contiene exactamente 41 tokens vÃ¡lidos.
âœ… Verifica que el corpus usa exclusivamente esos tokens.
âœ… Reporta cualquier carÃ¡cter fuera de vocabulario.
âœ… Verifica integridad general del corpus.

Uso:
    python scripts/verificar_corpus_lm08_integridad.py
"""

import json
from pathlib import Path
from collections import Counter
import unicodedata

# --- ConfiguraciÃ³n ---
VOCAB_PATH = Path("models/tokenizer_v2_base41/vocab.json")
CORPUS_PATH = Path("data/lm/corpus_lm08_total.txt")

# --- Cargar vocabulario ---
print(f"ğŸ“¦ Cargando vocabulario desde: {VOCAB_PATH}")
with VOCAB_PATH.open(encoding="utf-8") as f:
    vocab_json = json.load(f)

# Extraer caracteres reales del vocab
vocab_chars = set()
for token in vocab_json:
    if len(token) == 1:
        vocab_chars.add(token)

print(f"ğŸ”  Tokens Ãºnicos encontrados: {len(vocab_chars)}")
if len(vocab_chars) != 41:
    print(f"âŒ El vocabulario no tiene exactamente 41 tokens (encontrÃ³ {len(vocab_chars)})")
else:
    print("âœ… El vocabulario contiene exactamente 41 caracteres.")

# --- Verificar corpus ---
print(f"\nğŸ“– Verificando corpus: {CORPUS_PATH}")
total = 0
errores = 0
lineas_vacias = 0
caracteres_oov = Counter()

with CORPUS_PATH.open(encoding="utf-8") as f:
    for i, linea in enumerate(f, 1):
        linea = unicodedata.normalize("NFC", linea.strip())

        if not linea:
            lineas_vacias += 1
            continue

        for ch in linea:
            if ch not in vocab_chars:
                errores += 1
                caracteres_oov[ch] += 1

        total += 1
        if i % 500000 == 0:
            print(f"  â†’ {i:,} lÃ­neas procesadas â€¦")

# --- Resultados ---
print(f"\nğŸ“Š RESULTADOS")
print(f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"ğŸ“Œ LÃ­neas procesadas:    {total:,}")
print(f"âš ï¸  LÃ­neas vacÃ­as:        {lineas_vacias:,}")
print(f"â— Caracteres OOV:        {errores:,}")

if caracteres_oov:
    print("\nâ— Top 20 caracteres fuera del vocabulario:")
    for ch, count in caracteres_oov.most_common(20):
        nombre = unicodedata.name(ch, "UNKNOWN")
        print(f"  '{ch}' (U+{ord(ch):04X}): {count} veces â€“ {nombre}")
else:
    print("âœ… No se encontraron caracteres fuera del vocabulario.")

if errores == 0 and lineas_vacias == 0 and len(vocab_chars) == 41:
    print("\nâœ… El corpus es 100% compatible con tokenizer_v2_base41.")
else:
    print("\nâš ï¸  El corpus presenta inconsistencias. Revisa los detalles anteriores.")
