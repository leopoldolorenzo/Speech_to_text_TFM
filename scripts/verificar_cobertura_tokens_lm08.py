#!/usr/bin/env python
# -*- coding: utf-8 -*-

import json
from pathlib import Path

CORPUS_PATH = Path("data/lm/corpus_lm08_total.txt")
VOCAB_PATH = Path("models/tokenizer_v2_base41/vocab.json")

# Cargar vocabulario del tokenizer
with VOCAB_PATH.open(encoding="utf-8") as f:
    vocab = json.load(f)

tokens_validos = {t for t in vocab if len(t) == 1}  # Ignora tokens especiales (<s>, <pad>, etc.)

print(f"ğŸ“¦ Vocabulario cargado: {len(tokens_validos)} tokens vÃ¡lidos")
print(f"ğŸ”¤ Tokens vÃ¡lidos: {sorted(tokens_validos)}")

# Extraer caracteres Ãºnicos del corpus
caracteres_corpus = set()
with CORPUS_PATH.open(encoding="utf-8") as f:
    for linea in f:
        caracteres_corpus.update(set(linea.strip()))

print(f"\nğŸ“„ Tokens Ãºnicos en el corpus: {len(caracteres_corpus)}")
print(f"ğŸ”¡ Tokens encontrados: {sorted(caracteres_corpus)}")

# Verificaciones
fuera_vocab = caracteres_corpus - tokens_validos
no_usados = tokens_validos - caracteres_corpus

print("\nğŸ§ª VerificaciÃ³n:")
if fuera_vocab:
    print(f"âŒ Caracteres FUERA del vocabulario ({len(fuera_vocab)}): {sorted(fuera_vocab)}")
else:
    print("âœ… No hay caracteres fuera del vocabulario.")

if no_usados:
    print(f"âš ï¸  Tokens del vocabulario NO usados en el corpus ({len(no_usados)}): {sorted(no_usados)}")
else:
    print("âœ… Todos los tokens del vocabulario fueron usados al menos una vez.")

# Resultado final
if not fuera_vocab and not no_usados:
    print("\nğŸ¯ El corpus cubre perfectamente el vocabulario de 41 tokens y no contiene caracteres extraÃ±os.")
else:
    print("\nâš ï¸  El corpus NO cumple completamente con la cobertura esperada.")
