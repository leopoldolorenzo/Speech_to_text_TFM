import json
from pathlib import Path

# Ruta al archivo vocab.json
vocab_path = Path("models/tokenizer_v2_base41/vocab.json")

# Cargar el vocabulario
with open(vocab_path, "r", encoding="utf-8") as f:
    vocab = json.load(f)

# Verificar cantidad total de tokens
total_tokens = len(vocab)
print(f"ðŸ”¢ Total de tokens: {total_tokens}")

# Verificar existencia y posiciÃ³n de tokens especiales
expected_tokens = {
    "<pad>": 0,
    "<s>": 1,
    "</s>": 2,
    "<unk>": 3,
    "|": 4
}

print("\nðŸ§© Tokens especiales:")
for token, expected_index in expected_tokens.items():
    actual_index = vocab.get(token, None)
    status = "âœ…" if actual_index == expected_index else f"âŒ (encontrado en {actual_index})"
    print(f"{token}: esperado {expected_index} â†’ {status}")

# Mostrar ejemplo del vocabulario
print("\nðŸ“š Ejemplo del vocabulario:")
for char, idx in sorted(vocab.items(), key=lambda x: x[1]):
    print(f"{idx:2}: {char}")
    if idx >= 20:  # solo mostrar los primeros 20 tokens
        break
